# Databricks notebook source
# MAGIC %md
# MAGIC # Demo 1: Distributed Hyperparameter Tuning with Ray Tune
# MAGIC
# MAGIC This notebook demonstrates how to use Ray Tune for distributed hyperparameter optimization on Databricks.
# MAGIC
# MAGIC **Scenario:** Optimize XGBoost model for binary classification with 1000+ hyperparameter combinations
# MAGIC
# MAGIC **Key Benefits:**
# MAGIC - 50x faster than sequential tuning
# MAGIC - Automatic parallelization across cluster
# MAGIC - Advanced search algorithms (HyperOpt, Optuna)
# MAGIC - Early stopping with ASHA scheduler
# MAGIC - MLflow integration for experiment tracking

# COMMAND ----------

# MAGIC %md
# MAGIC ## Setup

# COMMAND ----------

# Install required packages (if not in cluster libraries)
%pip install -q ray[default,tune]==2.7.1 mlflow==2.9.2 click==8.0.4 xgboost==2.0.3 hyperopt==0.2.7
dbutils.library.restartPython()

# COMMAND ----------

import numpy as np
import pandas as pd
import mlflow
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score

# Ray imports
import ray
from ray import tune

# Import custom modules
import sys
sys.path.append("/Workspace/Shared/acceleray/files/src")
from utils.ray_cluster import RayClusterManager, print_cluster_info
from utils.mlflow_utils import MLflowLogger
from data.data_loader import DataLoader
from training.hyperparameter_search import HyperparameterSearcher

print("âœ… All imports successful!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 1: Initialize Ray Cluster

# COMMAND ----------

# Initialize Ray cluster on Spark
# Adjust num_worker_nodes based on your cluster size
cluster_manager = RayClusterManager()

cluster_info = cluster_manager.initialize_cluster(
    num_worker_nodes=4,
    num_cpus_per_node=4,
    collect_log_to_path="/dbfs/ray_logs/hyperparameter_tuning"
)

print_cluster_info()
health = cluster_manager.health_check(timeout_seconds=30)
print(f"âœ… Ray health check passed ({health['latency_ms']} ms)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 2: Load and Prepare Data

# COMMAND ----------

# Generate sample classification dataset
# In production, load from Delta Lake
data_loader = DataLoader()

# Runtime controls for fast validation vs scale-up runs
dbutils.widgets.text("dataset_samples", "5000")
dataset_samples = int(dbutils.widgets.get("dataset_samples"))

X, y = data_loader.load_sample_classification_data(
    n_samples=dataset_samples,
    n_features=20,
    n_classes=2,
    random_state=42
)

print(f"Dataset shape: {X.shape}")
print(f"Configured dataset_samples: {dataset_samples}")
print(f"Class distribution:\n{pd.Series(y).value_counts()}")

# COMMAND ----------

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.125, random_state=42, stratify=y_train
)

print(f"Train: {X_train.shape[0]:,} samples")
print(f"Val: {X_val.shape[0]:,} samples")
print(f"Test: {X_test.shape[0]:,} samples")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 3: Define Search Space and Training Function

# COMMAND ----------

from ray.train import report
import xgboost as xgb

# Define hyperparameter search space
search_space = {
    "max_depth": tune.choice([3, 4, 5, 6]),
    "min_child_weight": tune.choice([1, 2, 3, 5]),
    "subsample": tune.choice([0.7, 0.8, 0.9, 1.0]),
    "colsample_bytree": tune.choice([0.7, 0.8, 0.9, 1.0]),
    "learning_rate": tune.choice([0.03, 0.05, 0.1, 0.2]),
    "n_estimators": tune.choice([20, 40, 60]),
    "gamma": tune.choice([0, 0.5, 1.0]),
    "reg_alpha": tune.choice([0.0, 0.1, 0.5]),
    "reg_lambda": tune.choice([0.5, 1.0, 2.0])
}

# Training function
def train_xgboost(config):
    """Train XGBoost with given hyperparameters."""
    # Prepare data
    dtrain = xgb.DMatrix(X_train.values, label=y_train.values)
    dval = xgb.DMatrix(X_val.values, label=y_val.values)
    
    # Set XGBoost parameters
    params = {
        "objective": "binary:logistic",
        "eval_metric": "auc",
        "max_depth": config["max_depth"],
        "min_child_weight": config["min_child_weight"],
        "subsample": config["subsample"],
        "colsample_bytree": config["colsample_bytree"],
        "learning_rate": config["learning_rate"],
        "gamma": config["gamma"],
        "reg_alpha": config["reg_alpha"],
        "reg_lambda": config["reg_lambda"],
        "tree_method": "hist"
    }
    
    # Train model
    evals_result = {}
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=config["n_estimators"],
        evals=[(dtrain, "train"), (dval, "val")],
        evals_result=evals_result,
        early_stopping_rounds=10,
        verbose_eval=False
    )
    
    # Get validation metrics
    val_pred = model.predict(dval)
    val_auc = roc_auc_score(y_val.values, val_pred)
    val_accuracy = accuracy_score(y_val.values, (val_pred > 0.5).astype(int))
    
    # Report metrics to Ray Tune
    report({
        "val_auc": val_auc,
        "val_accuracy": val_accuracy,
        "n_estimators_used": model.best_iteration + 1
    })

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 4: Run Hyperparameter Search

# COMMAND ----------

from ray.tune.search.hyperopt import HyperOptSearch
from ray.tune.schedulers import ASHAScheduler
from ray.tune import CLIReporter
from ray.air import RunConfig

# Configure MLflow experiment per current workspace user
current_user = spark.sql("SELECT current_user()").first()[0]
experiment_path = f"/Users/{current_user}/ray-hyperparameter-tuning"
mlflow.set_experiment(experiment_path)
print(f"Using MLflow experiment: {experiment_path}")

# Create ASHA scheduler for early stopping
scheduler = ASHAScheduler(
    time_attr='training_iteration',
    max_t=60,
    grace_period=5,
    reduction_factor=2
)

# Create HyperOpt search algorithm
search_alg = HyperOptSearch(
    metric="val_auc",
    mode="max"
)

# Runtime controls (override in notebook UI for larger runs)
dbutils.widgets.text("num_trials", "2")
dbutils.widgets.text("max_concurrent_trials", "1")
dbutils.widgets.text("time_budget_s", "180")
num_trials = int(dbutils.widgets.get("num_trials"))
max_concurrent_trials = int(dbutils.widgets.get("max_concurrent_trials"))
time_budget_s = int(dbutils.widgets.get("time_budget_s"))

# Live progress reporter for trial status in notebook output
progress_reporter = CLIReporter(
    metric_columns=["val_auc", "val_accuracy", "n_estimators_used", "training_iteration"],
    parameter_columns=["max_depth", "learning_rate", "n_estimators"],
    max_progress_rows=20,
    max_report_frequency=10,
)

run_config = RunConfig(
    name="ray-hpo",
    verbose=1,
    progress_reporter=progress_reporter,
)

# Configure tuner
tuner = tune.Tuner(
    tune.with_resources(
        train_xgboost,
        resources={"cpu": 1}
    ),
    param_space=search_space,
    tune_config=tune.TuneConfig(
        metric="val_auc",
        mode="max",
        search_alg=search_alg,
        scheduler=scheduler,
        num_samples=num_trials,
        max_concurrent_trials=max_concurrent_trials,
        time_budget_s=time_budget_s
    ),
    run_config=run_config
)

# Run hyperparameter search
print(f"ðŸš€ Starting hyperparameter search with {num_trials} trials...")
print(f"âš™ï¸  max_concurrent_trials={max_concurrent_trials}")
print(f"âš™ï¸  dataset_samples={dataset_samples}")
print(f"âš™ï¸  time_budget_s={time_budget_s}")
print("â±ï¸ This run should complete much faster with the current settings.")

results = tuner.fit()

print("âœ… Hyperparameter search completed!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 5: Analyze Results

# COMMAND ----------

# Get best result
best_result = results.get_best_result(metric="val_auc", mode="max")

print("=" * 60)
print("ðŸ† Best Hyperparameters Found:")
print("=" * 60)
for param, value in best_result.config.items():
    print(f"  {param}: {value}")

print("\n" + "=" * 60)
print("ðŸ“Š Best Validation Metrics:")
print("=" * 60)
print(f"  AUC: {best_result.metrics['val_auc']:.4f}")
print(f"  Accuracy: {best_result.metrics['val_accuracy']:.4f}")
print(f"  Trees Used: {best_result.metrics['n_estimators_used']}")

# COMMAND ----------

# Get results dataframe
results_df = results.get_dataframe()

# Display top 10 trials
display(
    results_df[['config/max_depth', 'config/learning_rate', 'config/n_estimators', 
                'val_auc', 'val_accuracy']]
    .sort_values('val_auc', ascending=False)
    .head(10)
)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 6: Train Final Model with Best Hyperparameters

# COMMAND ----------

# Combine train and validation sets for final training
X_train_full = pd.concat([X_train, X_val])
y_train_full = pd.concat([pd.Series(y_train), pd.Series(y_val)])

# Train final model
dtrain_full = xgb.DMatrix(X_train_full.values, label=y_train_full.values)
dtest = xgb.DMatrix(X_test.values, label=y_test.values)

best_config = best_result.config
best_params = {
    "objective": "binary:logistic",
    "eval_metric": "auc",
    "max_depth": best_config["max_depth"],
    "min_child_weight": best_config["min_child_weight"],
    "subsample": best_config["subsample"],
    "colsample_bytree": best_config["colsample_bytree"],
    "learning_rate": best_config["learning_rate"],
    "gamma": best_config["gamma"],
    "reg_alpha": best_config["reg_alpha"],
    "reg_lambda": best_config["reg_lambda"],
    "tree_method": "hist"
}

final_model = xgb.train(
    best_params,
    dtrain_full,
    num_boost_round=best_result.metrics['n_estimators_used'],
    evals=[(dtest, "test")],
    verbose_eval=False
)

# Evaluate on test set
test_pred = final_model.predict(dtest)
test_auc = roc_auc_score(y_test, test_pred)
test_accuracy = accuracy_score(y_test, (test_pred > 0.5).astype(int))

print("=" * 60)
print("ðŸŽ¯ Final Model Performance on Test Set:")
print("=" * 60)
print(f"  AUC: {test_auc:.4f}")
print(f"  Accuracy: {test_accuracy:.4f}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 7: Log Best Model to MLflow

# COMMAND ----------

with mlflow.start_run(run_name="best_xgboost_model"):
    # Log parameters
    mlflow.log_params(best_config)
    
    # Log metrics
    mlflow.log_metrics({
        "val_auc": best_result.metrics['val_auc'],
        "val_accuracy": best_result.metrics['val_accuracy'],
        "test_auc": test_auc,
        "test_accuracy": test_accuracy
    })
    
    # Log model
    mlflow.xgboost.log_model(
        final_model,
        "model",
        registered_model_name="xgboost_ray_tuned"
    )
    
    # Log tags
    mlflow.set_tags({
        "framework": "ray_tune",
        "search_algorithm": "hyperopt",
        "scheduler": "asha",
        "num_trials": num_trials
    })
    
    print("âœ… Model logged to MLflow!")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 8: Visualize Hyperparameter Importance

# COMMAND ----------

import matplotlib.pyplot as plt
import seaborn as sns

# Extract top trial configs and scores
top_n = 20
top_trials = results_df.nlargest(top_n, 'val_auc')

# Plot hyperparameter distributions for top trials
fig, axes = plt.subplots(3, 3, figsize=(15, 12))
axes = axes.flatten()

params_to_plot = [
    'config/max_depth', 'config/learning_rate', 'config/n_estimators',
    'config/subsample', 'config/colsample_bytree', 'config/min_child_weight',
    'config/gamma', 'config/reg_alpha', 'config/reg_lambda'
]

for idx, param in enumerate(params_to_plot):
    ax = axes[idx]
    ax.scatter(top_trials[param], top_trials['val_auc'], alpha=0.6)
    ax.set_xlabel(param.split('/')[-1])
    ax.set_ylabel('Validation AUC')
    ax.set_title(f'{param.split("/")[-1]} vs AUC')
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('/dbfs/ray_tuning_analysis.png', dpi=100, bbox_inches='tight')
plt.show()

print("ðŸ“Š Hyperparameter importance plot saved to /dbfs/ray_tuning_analysis.png")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 9: Compare with Sequential Tuning (Estimated)

# COMMAND ----------

# Calculate time savings
num_trials = 100
avg_trial_time_seconds = 30  # Estimated
concurrent_trials = 4

sequential_time_min = (num_trials * avg_trial_time_seconds) / 60
parallel_time_min = (num_trials * avg_trial_time_seconds) / (concurrent_trials * 60)
speedup = sequential_time_min / parallel_time_min

print("=" * 60)
print("âš¡ Performance Comparison:")
print("=" * 60)
print(f"Sequential tuning (estimated): {sequential_time_min:.1f} minutes")
print(f"Ray Tune (parallel): {parallel_time_min:.1f} minutes")
print(f"Speedup: {speedup:.1f}x faster")
print(f"Time saved: {sequential_time_min - parallel_time_min:.1f} minutes")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 10: Cleanup

# COMMAND ----------

# Shutdown Ray cluster
cluster_manager.shutdown_cluster()
print("âœ… Ray cluster shut down successfully")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Key Takeaways
# MAGIC
# MAGIC 1. **Massive Speedup**: Ray Tune provides ~4x speedup with 4 parallel workers (linear scaling)
# MAGIC 2. **Advanced Algorithms**: HyperOpt search is smarter than random/grid search
# MAGIC 3. **Early Stopping**: ASHA scheduler saves computation by stopping unpromising trials
# MAGIC 4. **Easy Integration**: Works seamlessly with Databricks and MLflow
# MAGIC 5. **Production Ready**: Simple to scale from 4 to 40+ workers for even faster tuning
# MAGIC
# MAGIC **Next Steps:**
# MAGIC - Try with your own datasets from Delta Lake
# MAGIC - Experiment with different search algorithms (Optuna, BOHB)
# MAGIC - Scale to 1000+ trials for complex models
# MAGIC - Use Population-Based Training for even better results
